{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHmcc9Y9Zsim",
        "outputId": "c2838b75-5e2e-4813-97a7-6a6f679fec6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b7f86abed7c046c16f0c605ba617fa3359624d11e028c8dbac1602ae36ca4353\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "def matrix_vector_multiplication(matrix, vector):\n",
        "    sc = SparkContext(\"local\", \"MatrixVectorMultiplication\")\n",
        "    matrix_rdd = sc.parallelize(matrix)\n",
        "    vector_rdd = sc.parallelize(vector)\n",
        "\n",
        "    result = matrix_rdd.flatMap(lambda row: [(row[0], row[1][i]*vector[i]) for i in range(len(vector))]) \\\n",
        "                      .reduceByKey(lambda x, y: x + y) \\\n",
        "                      .sortByKey() \\\n",
        "                      .map(lambda x: x[1]) \\\n",
        "                      .collect()\n",
        "\n",
        "    sc.stop()\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "matrix = [(1, [1, 2, 3]), (2, [4, 5, 6]), (3, [7, 8, 9])]\n",
        "vector = [1, 2, 3]\n",
        "result = matrix_vector_multiplication(matrix, vector)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmke80ykZ2Q9",
        "outputId": "58ba7da6-66fe-48e8-92dc-9d6bd2b9c8d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 32, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "import numpy as np\n",
        "\n",
        "def aggregate_operations(data):\n",
        "    sc = SparkContext(\"local\", \"AggregateOperations\")\n",
        "    data_rdd = sc.parallelize(data)\n",
        "\n",
        "    mean = data_rdd.mean()\n",
        "    total_sum = data_rdd.sum()\n",
        "    std_dev = np.sqrt(data_rdd.map(lambda x: (x - mean)**2).sum() / data_rdd.count())\n",
        "\n",
        "    sc.stop()\n",
        "    return mean, total_sum, std_dev\n",
        "\n",
        "# Example usage\n",
        "data = [1, 2, 3, 4, 5]\n",
        "mean, total_sum, std_dev = aggregate_operations(data)\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Sum:\", total_sum)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAG-1PdqaCLe",
        "outputId": "ca37372e-4960-443b-9cac-8cb7d7524d82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 3.0\n",
            "Sum: 15\n",
            "Standard Deviation: 1.4142135623730951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "def sort_data(data):\n",
        "    # Check if a SparkContext already exists\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    # Create RDD from data\n",
        "    data_rdd = sc.parallelize(data)\n",
        "\n",
        "    # Sort the RDD\n",
        "    sorted_data = data_rdd.sortBy(lambda x: x)\n",
        "\n",
        "    # Stop SparkContext (if created in this function)\n",
        "    if not SparkContext._active_spark_context._jsc:\n",
        "        sc.stop()\n",
        "\n",
        "    # Collect and return sorted data\n",
        "    return sorted_data.collect()\n",
        "\n",
        "# Example usage\n",
        "data = [3, 1, 5, 2, 4]\n",
        "sorted_data = sort_data(data)\n",
        "print(sorted_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOZW-ZWeaH6P",
        "outputId": "6b368be4-99b9-4f15-9309-f138f3f843c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "def search_data(data, target):\n",
        "    # Check if a SparkContext already exists\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    # Create RDD from data\n",
        "    data_rdd = sc.parallelize(data)\n",
        "\n",
        "    # Check if target exists in data\n",
        "    found = data_rdd.filter(lambda x: x == target).count() > 0\n",
        "\n",
        "    # Stop SparkContext (if created in this function)\n",
        "    if not SparkContext._active_spark_context._jsc:\n",
        "        sc.stop()\n",
        "\n",
        "    return found\n",
        "\n",
        "# Example usage\n",
        "data = [1, 2, 3, 4, 5]\n",
        "target = 3\n",
        "result = search_data(data, target)\n",
        "print(\"Target found:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4PbmWLXav0B",
        "outputId": "bef946d4-7af6-4644-b545-65f374ca97fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target found: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "def map_side_join(data1, data2):\n",
        "    # Check if a SparkContext already exists\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    data1_rdd = sc.parallelize(data1)\n",
        "    data2_rdd = sc.parallelize(data2)\n",
        "\n",
        "    joined_data = data1_rdd.map(lambda x: (x[0], (x[1],))) \\\n",
        "                          .join(data2_rdd.map(lambda x: (x[0], (x[1],))))\n",
        "\n",
        "    return joined_data.collect()\n",
        "\n",
        "def reduce_side_join(data1, data2):\n",
        "    # Check if a SparkContext already exists\n",
        "    sc = SparkContext.getOrCreate()\n",
        "\n",
        "    data1_rdd = sc.parallelize(data1)\n",
        "    data2_rdd = sc.parallelize(data2)\n",
        "\n",
        "    joined_data = data1_rdd.join(data2_rdd)\n",
        "\n",
        "    return joined_data.collect()\n",
        "\n",
        "# Example usage\n",
        "data1 = [(1, 'A'), (2, 'B'), (3, 'C')]\n",
        "data2 = [(1, 'X'), (2, 'Y'), (4, 'Z')]\n",
        "map_side_result = map_side_join(data1, data2)\n",
        "reduce_side_result = reduce_side_join(data1, data2)\n",
        "\n",
        "print(\"Map Side Join Result:\", map_side_result)\n",
        "print(\"Reduce Side Join Result:\", reduce_side_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnWZX65KbSVC",
        "outputId": "6d4da0f0-8236-4034-9a40-28444401aa83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map Side Join Result: [(1, (('A',), ('X',))), (2, (('B',), ('Y',)))]\n",
            "Reduce Side Join Result: [(1, ('A', 'X')), (2, ('B', 'Y'))]\n"
          ]
        }
      ]
    }
  ]
}